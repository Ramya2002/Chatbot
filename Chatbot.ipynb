{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqN2Nx0UUv9mg1lGKWjSBc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQH8AQ_256Yi"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "import string\n",
        "import sklearn"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O0_Q0OK6K80",
        "outputId": "312adce1-6362-4478-b063-88cce6f62b9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f=open('NLP2.0 final.txt','r',errors='ignore')\n",
        "raw = f.read()\n",
        "\n",
        "#convert to lower case\n",
        "raw = raw.lower()\n",
        "\n",
        "#one time download\n",
        "nltk.download('punkt')\n",
        "\n",
        "#new library\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#raw data to sentences\n",
        "sent_tokens = nltk.sent_tokenize(raw)\n",
        "\n",
        "#raw data to list of words\n",
        "word_tokens = nltk.word_tokenize(raw)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhnIIWkh6lMM",
        "outputId": "5370c3c6-b763-4253-b1ae-935fe2748865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#example trial\n",
        "sent_tokens[:2]\n",
        "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.']\n",
        "word_tokens[:2]\n",
        "['natural','language']"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natural', 'language']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpmPaHDm6oWz"
      },
      "source": [
        "#preprocessing and handling raw data\n",
        "#Wordnet- english dictionary in nltk\n",
        "\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict ((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhzPWtB16xmn"
      },
      "source": [
        "#program a greet response\n",
        "\n",
        "greeting_inputs=(\"Hello\",\"Hi\",\"Hey there\",\"Whats up\",\"Heyy\",\"sup\",\"hi\")\n",
        "greeting_response=(\"Hello to you too\",\"Heyy\",\"Good to see you!\",\"Hey there!\",\"Howdy partner\")\n",
        "\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in greeting_inputs:\n",
        "            return random.choice(greeting_response)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyl3B8sI61xd"
      },
      "source": [
        "#generate response\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#cosine similarity to find similarities between input and raw doc\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXyyawZ365gT"
      },
      "source": [
        "#write the response function\n",
        "\n",
        "def response(user_response):\n",
        "  robo_response =''\n",
        "  sent_tokens.append(user_response)\n",
        "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize,stop_words ='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if (req_tfidf==0):\n",
        "    robo_response = robo_response+ \"I am sorry! I don't understand you\"\n",
        "    return robo_response\n",
        "  else:\n",
        "    robo_response = robo_response+sent_tokens[idx]\n",
        "    return robo_response"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kThNCN-68xP",
        "outputId": "4b9fbd1f-d988-4f2e-c1f1-65507ddd6735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#start and end points for conversation\n",
        "flag = True\n",
        "print(\"Maya:Hello there human. Today, I can tell you about NLP. If you want to leave, type Bye.\")\n",
        "while (flag == True):\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if (user_response != 'bye'):\n",
        "        if (user_response == 'thanks' or user_response=='thank you'):\n",
        "          flag = False\n",
        "          print(\"Maya:You're Welcome!\")\n",
        "        else:\n",
        "          if (greeting(user_response)!=None):\n",
        "              print(\"Maya:\"+greeting(user_response))\n",
        "          else:\n",
        "            print(\"Maya:\",end=\"\")\n",
        "            print(response(user_response))\n",
        "            sent_tokens.remove(user_response)\n",
        "        \n",
        "    else:\n",
        "          flag = False\n",
        "          print(\"Maya:Goodbye Mortal.Come back for more info\")\n",
        " \n",
        "    "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maya:Hello there human. Today, I can tell you about NLP. If you want to leave, type Bye.\n",
            "hi\n",
            "Maya:Hey there!\n",
            "what is nlp\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maya:natural language processing, nlp is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
            "what are the challenges it faces\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maya:challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.\n",
            "what is lemmatization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maya:I am sorry! I don't understand you\n",
            "what is sentiment analysis\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maya:sentiment analysis (see also multimodal sentiment analysis):extract subjective information usually from a set of documents, often using online reviews to determine \"polarity\" about specific objects.\n",
            "bye\n",
            "Maya:Goodbye Mortal.Come back for more info\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}